{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbd474a-ce79-4816-9cd2-310c3dca5ec2",
   "metadata": {},
   "source": [
    "This Jupyter Notebook allows various tests of the llm_demo sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210a1ac9-805f-4289-a2cb-787b1bde5fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-context learning using device:  cpu\n",
      "Fine-tuning using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ed7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ftparams = tests.Test_ft_params()\n",
    "test_ftparams.setUp()\n",
    "test_ftparams.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning entire models\n",
    "\n",
    "# test finetuing all parameters of bert-tiny with k=1\n",
    "%python3 main.py --task run_ft --model bert-tiny --dataset amazon --k 1\n",
    "\n",
    "# test finetuning multiple models with a range of k values, can take several hours on cpu\n",
    "%python3 main.py --task run_ft --model bert-tiny,bert-med --dataset amazon --k 1,8,128\n",
    "\n",
    "# plot finetuning results\n",
    "%python3 main.py --task plot_ft --model bert-tiny,bert-med --dataset amazon --k 1,8,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_test = tests.Test_ft()\n",
    "ft_test.setUp()\n",
    "ft_test.test_ft_modes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7206909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a minimal training loop to check for gradient tracking\n",
    "mode = \"last\"\n",
    "ft_test.test_gradient_tracking(mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53bbb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_icl = tests.Test_icl()\n",
    "test_icl.setUp()\n",
    "#test_icl.test_customPrompt_format()\n",
    "#test_icl.test_do_sample()\n",
    "test_icl.test_allPrompts_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in context learning on the bAbI dataset; gpt2-xl k=16 can take hours\n",
    "# choose yes when asked if wish to run the custom code\n",
    "%python3 main.py --task run_icl --model gpt2-med,gpt2-xl --dataset babi --k 0,1,16\n",
    "\n",
    "# plot icl results\n",
    "%python3 main.py --task plot_icl --model gpt2-med,gpt2-xl --dataset babi --k 0,1,16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test in context learning on the XSum dataset; can take hours on cpu\n",
    "%python3 main.py --task run_icl --model gpt2-med,gpt2-xl --dataset xsum --k 0,1,4 --prompt none,tldr,custom\n",
    "\n",
    "# plot icl results\n",
    "%python3 main.py --task plot_icl --model gpt2-med,gpt2-xl --dataset xsum --k 0,1,4 --prompt none,tldr,custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45533fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b6cec51",
   "metadata": {},
   "source": [
    "Test Multimodal Llama 3.2 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0b6d72",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "{'message': 'Invalid API key provided. You can find your API key at https://api.together.xyz/settings/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m load_env()\n\u001b[1;32m      7\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m   {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho wrote the book Charlotte\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Web?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m response_32 \u001b[38;5;241m=\u001b[39m \u001b[43mllama32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_32)\n",
      "File \u001b[0;32m/Volumes/SSD2TB/Google Drive/Work/MachineLearning/llm_demo/src/utilsLlama.py:95\u001b[0m, in \u001b[0;36mllama32\u001b[0;34m(messages, model_size)\u001b[0m\n\u001b[1;32m     92\u001b[0m res \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(requests\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(payload))\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[0;32m---> 95\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mException\u001b[0m: {'message': 'Invalid API key provided. You can find your API key at https://api.together.xyz/settings/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utilsLlama import load_env,llama32,llama31\n",
    "load_env()\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\",\n",
    "    \"content\": \"Who wrote the book Charlotte's Web?\"}\n",
    "]\n",
    "\n",
    "response_32 = llama32(messages, 90)\n",
    "print(response_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467159b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter REPLICATE API TOKEN to run inference\n",
    "REPLICATE_API_TOKEN = getpass(prompt=\"Enter REPLICATE API TOKEN: \")\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201120f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilsLlama\n",
    "\n",
    "testLlama = tests.Test_llama()\n",
    "\n",
    "# basic QA\n",
    "prompt = \"The typical color of a llama is: \"\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)\n",
    "\n",
    "# single turn chat\n",
    "prompt_chat = \"What is the average lifespan of a Llama? Answer the question in few words.\"\n",
    "output = testLlama.llama3_70b(prompt_chat)\n",
    "utilsLlama.md(output)\n",
    "\n",
    "# multi turn chat, with storing previous context\n",
    "prompt_chat = \"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: 15-20 years.\n",
    "User: What animal family are they?\n",
    "\"\"\"\n",
    "output = testLlama.llama3_70b(prompt_chat)\n",
    "utilsLlama.md(output)\n",
    "\n",
    "# multi-turn chat, with storing previous context and more instructions\n",
    "prompt_chat = \"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: Sure! The average lifespan of a llama is around 20-30 years.\n",
    "User: What animal family are they?\n",
    "\n",
    "Answer the question with one word.\n",
    "\"\"\"\n",
    "output = testLlama.llama3_70b(prompt_chat)\n",
    "utilsLlama.md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64472ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt engineering examples\n",
    "# Zero-shot example. To get positive/negative/neutral sentiment, we need to give examples in the prompt\n",
    "prompt = '''\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment: ?\n",
    "\n",
    "Give one word response.\n",
    "'''\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)\n",
    "\n",
    "# By giving examples to Llama, it understands the expected output format.\n",
    "prompt = '''\n",
    "Classify: I love Llamas!\n",
    "Sentiment: Positive\n",
    "Classify: I dont like Snakes.\n",
    "Sentiment: Negative\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment:\n",
    "\n",
    "Give one word response.\n",
    "'''\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df01345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain of thought examples\n",
    "# Standard prompting\n",
    "prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does Llama have?\n",
    "\n",
    "Answer in one word.\n",
    "'''\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)\n",
    "\n",
    "# By default, Llama 3 models follow \"Chain-Of-Thought\" prompting\n",
    "prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does Llama have?\n",
    "'''\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)\n",
    "\n",
    "# Chain of thought prompting with word problem\n",
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "Think step by step.\n",
    "Provide the answer as a single yes/no answer first.\n",
    "Then explain each intermediate step.\n",
    "\"\"\"\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fece411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Augmented Generation (RAG) using LangChain\n",
    "%pip install langchain\n",
    "%pip install langchain-community\n",
    "%pip install sentence-transformers\n",
    "%pip install faiss-cpu\n",
    "%pip install bs4\n",
    "%pip install langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94307efd",
   "metadata": {},
   "source": [
    "####  LangChain Q&A Retriever\n",
    "* ConversationalRetrievalChain\n",
    "* Query the Source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Step 1: Load the document from a web url\n",
    "loader = WebBaseLoader([\"https://huggingface.co/blog/llama31\"])\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split the document into chunks with a specified chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Store the document into a vector store with a specific embedding model\n",
    "vectorstore = FAISS.from_documents(all_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09e26c5",
   "metadata": {},
   "source": [
    "First sign in at [Groq](https://console.groq.com/login) with your github or gmail account, then get an API token to try Groq out for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "GROQ_API_TOKEN = getpass(prompt=\"Enter GROQ API TOKEN: \")\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d424829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Query against your own data\n",
    "chain = ConversationalRetrievalChain.from_llm(llm,vectorstore.as_retriever(),return_source_documents=True)\n",
    "\n",
    "# no chat history passed\n",
    "result = chain({\"question\": \"What’s new with Llama 3?\", \"chat_history\": []})\n",
    "utilsLlama.md(result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6836cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time your previous question and answer will be included as a chat history which will enable the ability\n",
    "# to ask follow up questions.\n",
    "query = \"What two sizes?\"\n",
    "chat_history = [(query, result[\"answer\"])]\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "utilsLlama.md(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbd474a-ce79-4816-9cd2-310c3dca5ec2",
   "metadata": {},
   "source": [
    "This Jupyter Notebook allows various tests of the llm_demo sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a1ac9-805f-4289-a2cb-787b1bde5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ed7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ftparams = tests.Test_ft_params()\n",
    "test_ftparams.setUp()\n",
    "test_ftparams.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning entire models\n",
    "\n",
    "# test finetuing all parameters of bert-tiny with k=1\n",
    "%python3 main.py --task run_ft --model bert-tiny --dataset amazon --k 1\n",
    "\n",
    "# test finetuning multiple models with a range of k values, can take several hours on cpu\n",
    "%python3 main.py --task run_ft --model bert-tiny,bert-med --dataset amazon --k 1,8,128\n",
    "\n",
    "# plot finetuning results\n",
    "%python3 main.py --task plot_ft --model bert-tiny,bert-med --dataset amazon --k 1,8,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_test = tests.Test_ft()\n",
    "ft_test.setUp()\n",
    "ft_test.test_ft_modes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7206909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a minimal training loop to check for gradient tracking\n",
    "mode = \"last\"\n",
    "ft_test.test_gradient_tracking(mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53bbb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_icl = tests.Test_icl()\n",
    "test_icl.setUp()\n",
    "#test_icl.test_customPrompt_format()\n",
    "#test_icl.test_do_sample()\n",
    "test_icl.test_allPrompts_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in context learning on the bAbI dataset; gpt2-xl k=16 can take hours\n",
    "# choose yes when asked if wish to run the custom code\n",
    "%python3 main.py --task run_icl --model gpt2-med,gpt2-xl --dataset babi --k 0,1,16\n",
    "\n",
    "# plot icl results\n",
    "%python3 main.py --task plot_icl --model gpt2-med,gpt2-xl --dataset babi --k 0,1,16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test in context learning on the XSum dataset; can take hours on cpu\n",
    "%python3 main.py --task run_icl --model gpt2-med,gpt2-xl --dataset xsum --k 0,1,4 --prompt none,tldr,custom\n",
    "\n",
    "# plot icl results\n",
    "%python3 main.py --task plot_icl --model gpt2-med,gpt2-xl --dataset xsum --k 0,1,4 --prompt none,tldr,custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45533fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b6cec51",
   "metadata": {},
   "source": [
    "Test Multimodal Llama 3.2 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utilsLlama import load_env,llama32,llama31\n",
    "load_env()\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\",\n",
    "    \"content\": \"Who wrote the book Charlotte's Web?\"}\n",
    "]\n",
    "\n",
    "response_32 = llama32(messages, 90)\n",
    "print(response_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467159b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter REPLICATE API TOKEN to run inference\n",
    "REPLICATE_API_TOKEN = getpass(prompt=\"Enter REPLICATE API TOKEN: \")\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201120f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilsLlama\n",
    "\n",
    "testLlama = tests.Test_llama()\n",
    "\n",
    "# basic QA\n",
    "prompt = \"The typical color of a llama is: \"\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)\n",
    "\n",
    "# single turn chat\n",
    "prompt_chat = \"What is the average lifespan of a Llama? Answer the question in few words.\"\n",
    "output = testLlama.llama3_70b(prompt_chat)\n",
    "utilsLlama.md(output)\n",
    "\n",
    "# multi turn chat, with storing previous context\n",
    "prompt_chat = \"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: 15-20 years.\n",
    "User: What animal family are they?\n",
    "\"\"\"\n",
    "output = testLlama.llama3_70b(prompt_chat)\n",
    "utilsLlama.md(output)\n",
    "\n",
    "# multi-turn chat, with storing previous context and more instructions\n",
    "prompt_chat = \"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: Sure! The average lifespan of a llama is around 20-30 years.\n",
    "User: What animal family are they?\n",
    "\n",
    "Answer the question with one word.\n",
    "\"\"\"\n",
    "output = testLlama.llama3_70b(prompt_chat)\n",
    "utilsLlama.md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64472ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt engineering examples\n",
    "# Zero-shot example. To get positive/negative/neutral sentiment, we need to give examples in the prompt\n",
    "prompt = '''\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment: ?\n",
    "\n",
    "Give one word response.\n",
    "'''\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)\n",
    "\n",
    "# By giving examples to Llama, it understands the expected output format.\n",
    "prompt = '''\n",
    "Classify: I love Llamas!\n",
    "Sentiment: Positive\n",
    "Classify: I dont like Snakes.\n",
    "Sentiment: Negative\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment:\n",
    "\n",
    "Give one word response.\n",
    "'''\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df01345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain of thought examples\n",
    "# Standard prompting\n",
    "prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does Llama have?\n",
    "\n",
    "Answer in one word.\n",
    "'''\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)\n",
    "\n",
    "# By default, Llama 3 models follow \"Chain-Of-Thought\" prompting\n",
    "prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does Llama have?\n",
    "'''\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)\n",
    "\n",
    "# Chain of thought prompting with word problem\n",
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "Think step by step.\n",
    "Provide the answer as a single yes/no answer first.\n",
    "Then explain each intermediate step.\n",
    "\"\"\"\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fece411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Augmented Generation (RAG) using LangChain\n",
    "%pip install langchain\n",
    "%pip install langchain-community\n",
    "%pip install sentence-transformers\n",
    "%pip install faiss-cpu\n",
    "%pip install bs4\n",
    "%pip install langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94307efd",
   "metadata": {},
   "source": [
    "####  LangChain Q&A Retriever\n",
    "* ConversationalRetrievalChain\n",
    "* Query the Source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Step 1: Load the document from a web url\n",
    "loader = WebBaseLoader([\"https://huggingface.co/blog/llama31\"])\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split the document into chunks with a specified chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Store the document into a vector store with a specific embedding model\n",
    "vectorstore = FAISS.from_documents(all_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09e26c5",
   "metadata": {},
   "source": [
    "First sign in at [Groq](https://console.groq.com/login) with your github or gmail account, then get an API token to try Groq out for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "GROQ_API_TOKEN = getpass(prompt=\"Enter GROQ API TOKEN: \")\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_TOKEN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

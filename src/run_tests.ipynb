{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbd474a-ce79-4816-9cd2-310c3dca5ec2",
   "metadata": {},
   "source": [
    "This Jupyter Notebook allows various tests of the llm_demo sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a1ac9-805f-4289-a2cb-787b1bde5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ed7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ftparams = tests.Test_ft_params()\n",
    "test_ftparams.setUp()\n",
    "test_ftparams.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning entire models\n",
    "\n",
    "# test finetuing all parameters of bert-tiny with k=1\n",
    "%python3 main.py --task run_ft --model bert-tiny --dataset amazon --k 1\n",
    "\n",
    "# test finetuning multiple models with a range of k values, can take several hours on cpu\n",
    "%python3 main.py --task run_ft --model bert-tiny,bert-med --dataset amazon --k 1,8,128\n",
    "\n",
    "# plot finetuning results\n",
    "%python3 main.py --task plot_ft --model bert-tiny,bert-med --dataset amazon --k 1,8,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_test = tests.Test_ft()\n",
    "ft_test.setUp()\n",
    "ft_test.test_ft_modes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7206909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a minimal training loop to check for gradient tracking\n",
    "mode = \"last\"\n",
    "ft_test.test_gradient_tracking(mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53bbb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_icl = tests.Test_icl()\n",
    "test_icl.setUp()\n",
    "#test_icl.test_customPrompt_format()\n",
    "#test_icl.test_do_sample()\n",
    "test_icl.test_allPrompts_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in context learning on the bAbI dataset; gpt2-xl k=16 can take hours\n",
    "# choose yes when asked if wish to run the custom code\n",
    "%python3 main.py --task run_icl --model gpt2-med,gpt2-xl --dataset babi --k 0,1,16\n",
    "\n",
    "# plot icl results\n",
    "%python3 main.py --task plot_icl --model gpt2-med,gpt2-xl --dataset babi --k 0,1,16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test in context learning on the XSum dataset; can take hours on cpu\n",
    "%python3 main.py --task run_icl --model gpt2-med,gpt2-xl --dataset xsum --k 0,1,4 --prompt none,tldr,custom\n",
    "\n",
    "# plot icl results\n",
    "%python3 main.py --task plot_icl --model gpt2-med,gpt2-xl --dataset xsum --k 0,1,4 --prompt none,tldr,custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45533fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b6cec51",
   "metadata": {},
   "source": [
    "Test Multimodal Llama 3.2 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utilsLlama import load_env,llama32,llama31\n",
    "load_env()\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\",\n",
    "    \"content\": \"Who wrote the book Charlotte's Web?\"}\n",
    "]\n",
    "\n",
    "response_32 = llama32(messages, 90)\n",
    "print(response_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467159b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter REPLICATE API TOKEN to run inference\n",
    "REPLICATE_API_TOKEN = getpass(prompt=\"Enter REPLICATE API TOKEN: \")\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201120f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilsLlama\n",
    "\n",
    "testLlama = tests.Test_llama()\n",
    "\n",
    "prompt = \"The typical color of a llama is: \"\n",
    "output = testLlama.llama3_70b(prompt)\n",
    "utilsLlama.md(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
